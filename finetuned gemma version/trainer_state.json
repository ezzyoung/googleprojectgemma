{
  "best_metric": null,
  "best_model_checkpoint": null,
  "epoch": 0.0026657603081618915,
  "eval_steps": 500,
  "global_step": 100,
  "is_hyper_param_search": false,
  "is_local_process_zero": true,
  "is_world_process_zero": true,
  "log_history": [
    {
      "epoch": 2.6657603081618918e-05,
      "grad_norm": NaN,
      "learning_rate": 0.0,
      "loss": 2.9918,
      "step": 1
    },
    {
      "epoch": 5.3315206163237835e-05,
      "grad_norm": NaN,
      "learning_rate": 0.0,
      "loss": 2.1588,
      "step": 2
    },
    {
      "epoch": 7.997280924485675e-05,
      "grad_norm": 23.823251724243164,
      "learning_rate": 4.000000000000001e-06,
      "loss": 1.5573,
      "step": 3
    },
    {
      "epoch": 0.00010663041232647567,
      "grad_norm": NaN,
      "learning_rate": 4.000000000000001e-06,
      "loss": 2.7518,
      "step": 4
    },
    {
      "epoch": 0.00013328801540809457,
      "grad_norm": 36.26675033569336,
      "learning_rate": 8.000000000000001e-06,
      "loss": 2.1587,
      "step": 5
    },
    {
      "epoch": 0.0001599456184897135,
      "grad_norm": 36.10761642456055,
      "learning_rate": 1.2e-05,
      "loss": 2.0384,
      "step": 6
    },
    {
      "epoch": 0.0001866032215713324,
      "grad_norm": 38.92612838745117,
      "learning_rate": 1.6000000000000003e-05,
      "loss": 2.0603,
      "step": 7
    },
    {
      "epoch": 0.00021326082465295134,
      "grad_norm": 31.426523208618164,
      "learning_rate": 2e-05,
      "loss": 1.4759,
      "step": 8
    },
    {
      "epoch": 0.00023991842773457025,
      "grad_norm": 19.616416931152344,
      "learning_rate": 1.9789473684210528e-05,
      "loss": 1.1882,
      "step": 9
    },
    {
      "epoch": 0.00026657603081618915,
      "grad_norm": 32.888858795166016,
      "learning_rate": 1.9578947368421055e-05,
      "loss": 2.3095,
      "step": 10
    },
    {
      "epoch": 0.00029323363389780805,
      "grad_norm": 26.843168258666992,
      "learning_rate": 1.936842105263158e-05,
      "loss": 0.9489,
      "step": 11
    },
    {
      "epoch": 0.000319891236979427,
      "grad_norm": 34.7044792175293,
      "learning_rate": 1.9157894736842108e-05,
      "loss": 2.3796,
      "step": 12
    },
    {
      "epoch": 0.0003465488400610459,
      "grad_norm": NaN,
      "learning_rate": 1.9157894736842108e-05,
      "loss": 3.0016,
      "step": 13
    },
    {
      "epoch": 0.0003732064431426648,
      "grad_norm": 55.44026184082031,
      "learning_rate": 1.894736842105263e-05,
      "loss": 3.9259,
      "step": 14
    },
    {
      "epoch": 0.0003998640462242837,
      "grad_norm": 32.35049057006836,
      "learning_rate": 1.873684210526316e-05,
      "loss": 1.3728,
      "step": 15
    },
    {
      "epoch": 0.0004265216493059027,
      "grad_norm": 40.703433990478516,
      "learning_rate": 1.8526315789473684e-05,
      "loss": 2.8196,
      "step": 16
    },
    {
      "epoch": 0.0004531792523875216,
      "grad_norm": 40.50764083862305,
      "learning_rate": 1.831578947368421e-05,
      "loss": 2.5053,
      "step": 17
    },
    {
      "epoch": 0.0004798368554691405,
      "grad_norm": 33.92988586425781,
      "learning_rate": 1.810526315789474e-05,
      "loss": 2.3521,
      "step": 18
    },
    {
      "epoch": 0.0005064944585507594,
      "grad_norm": 23.184751510620117,
      "learning_rate": 1.7894736842105264e-05,
      "loss": 1.7117,
      "step": 19
    },
    {
      "epoch": 0.0005331520616323783,
      "grad_norm": 30.867765426635742,
      "learning_rate": 1.768421052631579e-05,
      "loss": 1.3404,
      "step": 20
    },
    {
      "epoch": 0.0005598096647139973,
      "grad_norm": 35.6750602722168,
      "learning_rate": 1.7473684210526317e-05,
      "loss": 2.536,
      "step": 21
    },
    {
      "epoch": 0.0005864672677956161,
      "grad_norm": 44.620094299316406,
      "learning_rate": 1.7263157894736843e-05,
      "loss": 2.4265,
      "step": 22
    },
    {
      "epoch": 0.0006131248708772351,
      "grad_norm": 29.060096740722656,
      "learning_rate": 1.705263157894737e-05,
      "loss": 1.5808,
      "step": 23
    },
    {
      "epoch": 0.000639782473958854,
      "grad_norm": 43.08716583251953,
      "learning_rate": 1.6842105263157896e-05,
      "loss": 2.6174,
      "step": 24
    },
    {
      "epoch": 0.0006664400770404729,
      "grad_norm": 47.135398864746094,
      "learning_rate": 1.6631578947368423e-05,
      "loss": 3.537,
      "step": 25
    },
    {
      "epoch": 0.0006930976801220918,
      "grad_norm": 38.434452056884766,
      "learning_rate": 1.642105263157895e-05,
      "loss": 4.2312,
      "step": 26
    },
    {
      "epoch": 0.0007197552832037108,
      "grad_norm": 52.2720947265625,
      "learning_rate": 1.6210526315789473e-05,
      "loss": 5.5288,
      "step": 27
    },
    {
      "epoch": 0.0007464128862853296,
      "grad_norm": 33.54786682128906,
      "learning_rate": 1.6000000000000003e-05,
      "loss": 1.7628,
      "step": 28
    },
    {
      "epoch": 0.0007730704893669486,
      "grad_norm": 28.5720157623291,
      "learning_rate": 1.578947368421053e-05,
      "loss": 1.3443,
      "step": 29
    },
    {
      "epoch": 0.0007997280924485674,
      "grad_norm": 30.99793243408203,
      "learning_rate": 1.5578947368421052e-05,
      "loss": 1.6862,
      "step": 30
    },
    {
      "epoch": 0.0008263856955301864,
      "grad_norm": 26.623411178588867,
      "learning_rate": 1.536842105263158e-05,
      "loss": 2.2453,
      "step": 31
    },
    {
      "epoch": 0.0008530432986118054,
      "grad_norm": 30.206974029541016,
      "learning_rate": 1.5157894736842107e-05,
      "loss": 1.9826,
      "step": 32
    },
    {
      "epoch": 0.0008797009016934242,
      "grad_norm": 25.386507034301758,
      "learning_rate": 1.4947368421052632e-05,
      "loss": 3.43,
      "step": 33
    },
    {
      "epoch": 0.0009063585047750432,
      "grad_norm": 34.05515670776367,
      "learning_rate": 1.4736842105263159e-05,
      "loss": 2.6295,
      "step": 34
    },
    {
      "epoch": 0.000933016107856662,
      "grad_norm": 27.858686447143555,
      "learning_rate": 1.4526315789473687e-05,
      "loss": 2.3484,
      "step": 35
    },
    {
      "epoch": 0.000959673710938281,
      "grad_norm": 24.32611846923828,
      "learning_rate": 1.4315789473684212e-05,
      "loss": 2.3442,
      "step": 36
    },
    {
      "epoch": 0.0009863313140199,
      "grad_norm": 24.31877326965332,
      "learning_rate": 1.4105263157894738e-05,
      "loss": 2.1083,
      "step": 37
    },
    {
      "epoch": 0.001012988917101519,
      "grad_norm": 34.73572540283203,
      "learning_rate": 1.3894736842105265e-05,
      "loss": 1.8818,
      "step": 38
    },
    {
      "epoch": 0.0010396465201831376,
      "grad_norm": 28.350828170776367,
      "learning_rate": 1.3684210526315791e-05,
      "loss": 2.1268,
      "step": 39
    },
    {
      "epoch": 0.0010663041232647566,
      "grad_norm": 33.70315170288086,
      "learning_rate": 1.3473684210526316e-05,
      "loss": 2.1923,
      "step": 40
    },
    {
      "epoch": 0.0010929617263463756,
      "grad_norm": 23.14655303955078,
      "learning_rate": 1.3263157894736843e-05,
      "loss": 1.7703,
      "step": 41
    },
    {
      "epoch": 0.0011196193294279945,
      "grad_norm": 25.47197151184082,
      "learning_rate": 1.305263157894737e-05,
      "loss": 2.4542,
      "step": 42
    },
    {
      "epoch": 0.0011462769325096135,
      "grad_norm": 33.68373107910156,
      "learning_rate": 1.2842105263157896e-05,
      "loss": 2.5843,
      "step": 43
    },
    {
      "epoch": 0.0011729345355912322,
      "grad_norm": 27.25079345703125,
      "learning_rate": 1.263157894736842e-05,
      "loss": 1.8569,
      "step": 44
    },
    {
      "epoch": 0.0011995921386728512,
      "grad_norm": 34.77565002441406,
      "learning_rate": 1.2421052631578949e-05,
      "loss": 3.261,
      "step": 45
    },
    {
      "epoch": 0.0012262497417544701,
      "grad_norm": 30.122331619262695,
      "learning_rate": 1.2210526315789475e-05,
      "loss": 2.7719,
      "step": 46
    },
    {
      "epoch": 0.001252907344836089,
      "grad_norm": 33.611454010009766,
      "learning_rate": 1.2e-05,
      "loss": 4.0993,
      "step": 47
    },
    {
      "epoch": 0.001279564947917708,
      "grad_norm": 25.108505249023438,
      "learning_rate": 1.1789473684210527e-05,
      "loss": 1.2076,
      "step": 48
    },
    {
      "epoch": 0.0013062225509993268,
      "grad_norm": 33.54696273803711,
      "learning_rate": 1.1578947368421053e-05,
      "loss": 2.0784,
      "step": 49
    },
    {
      "epoch": 0.0013328801540809457,
      "grad_norm": 24.430782318115234,
      "learning_rate": 1.136842105263158e-05,
      "loss": 1.991,
      "step": 50
    },
    {
      "epoch": 0.0013595377571625647,
      "grad_norm": 26.002466201782227,
      "learning_rate": 1.1157894736842105e-05,
      "loss": 2.3739,
      "step": 51
    },
    {
      "epoch": 0.0013861953602441837,
      "grad_norm": 33.60374069213867,
      "learning_rate": 1.0947368421052633e-05,
      "loss": 2.1883,
      "step": 52
    },
    {
      "epoch": 0.0014128529633258026,
      "grad_norm": 34.90305709838867,
      "learning_rate": 1.073684210526316e-05,
      "loss": 3.1526,
      "step": 53
    },
    {
      "epoch": 0.0014395105664074216,
      "grad_norm": 25.343839645385742,
      "learning_rate": 1.0526315789473684e-05,
      "loss": 1.282,
      "step": 54
    },
    {
      "epoch": 0.0014661681694890403,
      "grad_norm": 25.354324340820312,
      "learning_rate": 1.0315789473684213e-05,
      "loss": 1.9357,
      "step": 55
    },
    {
      "epoch": 0.0014928257725706593,
      "grad_norm": 31.74823760986328,
      "learning_rate": 1.0105263157894738e-05,
      "loss": 2.7349,
      "step": 56
    },
    {
      "epoch": 0.0015194833756522782,
      "grad_norm": 34.03422927856445,
      "learning_rate": 9.894736842105264e-06,
      "loss": 2.6237,
      "step": 57
    },
    {
      "epoch": 0.0015461409787338972,
      "grad_norm": 28.341236114501953,
      "learning_rate": 9.68421052631579e-06,
      "loss": 1.8651,
      "step": 58
    },
    {
      "epoch": 0.0015727985818155162,
      "grad_norm": 35.74855041503906,
      "learning_rate": 9.473684210526315e-06,
      "loss": 1.9295,
      "step": 59
    },
    {
      "epoch": 0.001599456184897135,
      "grad_norm": 28.331565856933594,
      "learning_rate": 9.263157894736842e-06,
      "loss": 2.0254,
      "step": 60
    },
    {
      "epoch": 0.0016261137879787539,
      "grad_norm": 25.268508911132812,
      "learning_rate": 9.05263157894737e-06,
      "loss": 1.0968,
      "step": 61
    },
    {
      "epoch": 0.0016527713910603728,
      "grad_norm": 45.05856704711914,
      "learning_rate": 8.842105263157895e-06,
      "loss": 3.0862,
      "step": 62
    },
    {
      "epoch": 0.0016794289941419918,
      "grad_norm": 27.233692169189453,
      "learning_rate": 8.631578947368422e-06,
      "loss": 1.5873,
      "step": 63
    },
    {
      "epoch": 0.0017060865972236107,
      "grad_norm": 27.866178512573242,
      "learning_rate": 8.421052631578948e-06,
      "loss": 2.8118,
      "step": 64
    },
    {
      "epoch": 0.0017327442003052295,
      "grad_norm": 38.2933349609375,
      "learning_rate": 8.210526315789475e-06,
      "loss": 1.7898,
      "step": 65
    },
    {
      "epoch": 0.0017594018033868484,
      "grad_norm": 34.0709114074707,
      "learning_rate": 8.000000000000001e-06,
      "loss": 1.265,
      "step": 66
    },
    {
      "epoch": 0.0017860594064684674,
      "grad_norm": 30.964452743530273,
      "learning_rate": 7.789473684210526e-06,
      "loss": 2.6248,
      "step": 67
    },
    {
      "epoch": 0.0018127170095500863,
      "grad_norm": 39.841224670410156,
      "learning_rate": 7.578947368421054e-06,
      "loss": 2.9164,
      "step": 68
    },
    {
      "epoch": 0.0018393746126317053,
      "grad_norm": 42.51334762573242,
      "learning_rate": 7.368421052631579e-06,
      "loss": 2.6259,
      "step": 69
    },
    {
      "epoch": 0.001866032215713324,
      "grad_norm": 39.668739318847656,
      "learning_rate": 7.157894736842106e-06,
      "loss": 2.1014,
      "step": 70
    },
    {
      "epoch": 0.001892689818794943,
      "grad_norm": 37.67365264892578,
      "learning_rate": 6.947368421052632e-06,
      "loss": 2.7171,
      "step": 71
    },
    {
      "epoch": 0.001919347421876562,
      "grad_norm": 25.33510971069336,
      "learning_rate": 6.736842105263158e-06,
      "loss": 1.5196,
      "step": 72
    },
    {
      "epoch": 0.001946005024958181,
      "grad_norm": 39.3066520690918,
      "learning_rate": 6.526315789473685e-06,
      "loss": 2.5185,
      "step": 73
    },
    {
      "epoch": 0.0019726626280398,
      "grad_norm": 27.699329376220703,
      "learning_rate": 6.31578947368421e-06,
      "loss": 2.0521,
      "step": 74
    },
    {
      "epoch": 0.0019993202311214186,
      "grad_norm": 38.79856872558594,
      "learning_rate": 6.105263157894738e-06,
      "loss": 3.8212,
      "step": 75
    },
    {
      "epoch": 0.002025977834203038,
      "grad_norm": 36.98831558227539,
      "learning_rate": 5.8947368421052634e-06,
      "loss": 2.5078,
      "step": 76
    },
    {
      "epoch": 0.0020526354372846565,
      "grad_norm": 25.675683975219727,
      "learning_rate": 5.68421052631579e-06,
      "loss": 1.3956,
      "step": 77
    },
    {
      "epoch": 0.0020792930403662753,
      "grad_norm": 28.48395538330078,
      "learning_rate": 5.4736842105263165e-06,
      "loss": 2.5234,
      "step": 78
    },
    {
      "epoch": 0.0021059506434478945,
      "grad_norm": 24.80713653564453,
      "learning_rate": 5.263157894736842e-06,
      "loss": 1.4549,
      "step": 79
    },
    {
      "epoch": 0.002132608246529513,
      "grad_norm": 27.52273941040039,
      "learning_rate": 5.052631578947369e-06,
      "loss": 2.4111,
      "step": 80
    },
    {
      "epoch": 0.0021592658496111324,
      "grad_norm": NaN,
      "learning_rate": 5.052631578947369e-06,
      "loss": 1.7727,
      "step": 81
    },
    {
      "epoch": 0.002185923452692751,
      "grad_norm": 49.33625411987305,
      "learning_rate": 4.842105263157895e-06,
      "loss": 2.1395,
      "step": 82
    },
    {
      "epoch": 0.00221258105577437,
      "grad_norm": 23.49542236328125,
      "learning_rate": 4.631578947368421e-06,
      "loss": 2.1904,
      "step": 83
    },
    {
      "epoch": 0.002239238658855989,
      "grad_norm": 30.494853973388672,
      "learning_rate": 4.4210526315789476e-06,
      "loss": 2.7617,
      "step": 84
    },
    {
      "epoch": 0.0022658962619376078,
      "grad_norm": 38.32707214355469,
      "learning_rate": 4.210526315789474e-06,
      "loss": 3.6085,
      "step": 85
    },
    {
      "epoch": 0.002292553865019227,
      "grad_norm": 44.53705978393555,
      "learning_rate": 4.000000000000001e-06,
      "loss": 3.1759,
      "step": 86
    },
    {
      "epoch": 0.0023192114681008457,
      "grad_norm": 31.421253204345703,
      "learning_rate": 3.789473684210527e-06,
      "loss": 2.9896,
      "step": 87
    },
    {
      "epoch": 0.0023458690711824644,
      "grad_norm": 35.784263610839844,
      "learning_rate": 3.578947368421053e-06,
      "loss": 3.0205,
      "step": 88
    },
    {
      "epoch": 0.0023725266742640836,
      "grad_norm": 24.741188049316406,
      "learning_rate": 3.368421052631579e-06,
      "loss": 1.5578,
      "step": 89
    },
    {
      "epoch": 0.0023991842773457023,
      "grad_norm": 32.76400375366211,
      "learning_rate": 3.157894736842105e-06,
      "loss": 2.2646,
      "step": 90
    },
    {
      "epoch": 0.0024258418804273215,
      "grad_norm": 27.729089736938477,
      "learning_rate": 2.9473684210526317e-06,
      "loss": 2.5328,
      "step": 91
    },
    {
      "epoch": 0.0024524994835089403,
      "grad_norm": 40.413848876953125,
      "learning_rate": 2.7368421052631583e-06,
      "loss": 2.5032,
      "step": 92
    },
    {
      "epoch": 0.002479157086590559,
      "grad_norm": 26.81645965576172,
      "learning_rate": 2.5263157894736844e-06,
      "loss": 2.4346,
      "step": 93
    },
    {
      "epoch": 0.002505814689672178,
      "grad_norm": 35.21698760986328,
      "learning_rate": 2.3157894736842105e-06,
      "loss": 2.8967,
      "step": 94
    },
    {
      "epoch": 0.002532472292753797,
      "grad_norm": 33.091461181640625,
      "learning_rate": 2.105263157894737e-06,
      "loss": 2.5666,
      "step": 95
    },
    {
      "epoch": 0.002559129895835416,
      "grad_norm": 37.53681564331055,
      "learning_rate": 1.8947368421052634e-06,
      "loss": 3.055,
      "step": 96
    },
    {
      "epoch": 0.002585787498917035,
      "grad_norm": 39.260868072509766,
      "learning_rate": 1.6842105263157895e-06,
      "loss": 2.2491,
      "step": 97
    },
    {
      "epoch": 0.0026124451019986536,
      "grad_norm": 36.2708854675293,
      "learning_rate": 1.4736842105263159e-06,
      "loss": 4.0123,
      "step": 98
    },
    {
      "epoch": 0.0026391027050802728,
      "grad_norm": 29.414209365844727,
      "learning_rate": 1.2631578947368422e-06,
      "loss": 2.3005,
      "step": 99
    },
    {
      "epoch": 0.0026657603081618915,
      "grad_norm": 22.667869567871094,
      "learning_rate": 1.0526315789473685e-06,
      "loss": 0.5787,
      "step": 100
    }
  ],
  "logging_steps": 1,
  "max_steps": 100,
  "num_input_tokens_seen": 0,
  "num_train_epochs": 1,
  "save_steps": 500,
  "stateful_callbacks": {
    "TrainerControl": {
      "args": {
        "should_epoch_stop": false,
        "should_evaluate": false,
        "should_log": false,
        "should_save": true,
        "should_training_stop": true
      },
      "attributes": {}
    }
  },
  "total_flos": 5026525426483200.0,
  "train_batch_size": 2,
  "trial_name": null,
  "trial_params": null
}
